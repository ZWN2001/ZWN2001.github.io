<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>cs224w-图机器学习1 | ZWN's blog</title><meta name="author" content="琉璃月"><meta name="copyright" content="琉璃月"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="cs224w-图机器学习1 Introduction 选择图的原因：图是用于描述并分析有关联 &#x2F; 互动的实体的一种普适语言。它不将实体视为一系列孤立的点，而认为其互相之间有关系。它是一种很好的描述领域知识的方式。 网络与图的分类  networks &#x2F; natural graphs：自然表示为图  Social networks: Society is a collection of 7+ bil"><meta property="og:type" content="article"><meta property="og:title" content="cs224w-图机器学习1"><meta property="og:url" content="https://zwn2001.space/posts/cs224w-%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/index.html"><meta property="og:site_name" content="ZWN&#39;s blog"><meta property="og:description" content="cs224w-图机器学习1 Introduction 选择图的原因：图是用于描述并分析有关联 &#x2F; 互动的实体的一种普适语言。它不将实体视为一系列孤立的点，而认为其互相之间有关系。它是一种很好的描述领域知识的方式。 网络与图的分类  networks &#x2F; natural graphs：自然表示为图  Social networks: Society is a collection of 7+ bil"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://zwn2001.space/img/cover/19.jpg"><meta property="article:published_time" content="2024-08-14T01:41:41.000Z"><meta property="article:modified_time" content="2024-08-14T04:19:24.078Z"><meta property="article:author" content="琉璃月"><meta property="article:tag" content="图机器学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://zwn2001.space/img/cover/19.jpg"><link rel="shortcut icon" href="/img/favicon.webp"><link rel="canonical" href="https://zwn2001.space/posts/cs224w-%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://unpkg.com/@fancyapps/ui/dist/fancybox/fancybox.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!0,top_n_per_article:-1,unescape:!1,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:200,position:"top",messagePrev:"距离上次更新已经过去",messageNext:"天啦！注意内容可能过时。"},highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:300},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,source:{justifiedGallery:{js:"https://unpkg.com/flickr-justified-gallery/dist/fjGallery.min.js",css:"https://unpkg.com/flickr-justified-gallery/dist/fjGallery.css"}},isPhotoFigcaption:!0,islazyload:!1,isAnchor:!0,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"cs224w-图机器学习1",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-08-14 12:19:24"}</script><noscript><style type="text/css">#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,a){0!==a&&(a=864e5*a,t={value:t,expiry:(new Date).getTime()+a},localStorage.setItem(e,JSON.stringify(t)))},get:function(e){var t=localStorage.getItem(e);if(t){t=JSON.parse(t);if(!((new Date).getTime()>t.expiry))return t.value;localStorage.removeItem(e)}}},e.getScript=o=>new Promise((t,e)=>{const a=document.createElement("script");a.src=o,a.async=!0,a.onerror=e,a.onload=a.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)}),e.getCSS=(o,n=!1)=>new Promise((t,e)=>{const a=document.createElement("link");a.rel="stylesheet",a.href=o,n&&(a.id=n),a.onerror=e,a.onload=a.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","ffffff")};e=saveToLocal.get("theme"),"dark"===e?activateDarkMode():"light"===e&&activateLightMode(),e=saveToLocal.get("aside-status");void 0!==e&&("hide"===e?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/transpancy.css"><link rel="stylesheet" href="/css/iconfont.css"><link rel="stylesheet" href="/css/rightmenu.css"><link rel="stylesheet" href="/css/loadimg.css"><link rel="stylesheet" href="/css/project.css"><link type="text/html" rel="stylesheet" href="/css/wide_screen.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"><style>#recent-posts>.recent-post-item>.recent-post-info>.article-meta-wrap>.tags:before{content:"\A";white-space:pre}#recent-posts>.recent-post-item>.recent-post-info>.article-meta-wrap>.tags>.article-meta__separator{display:none}</style><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" src="/img/favicon.webp"><div class="loading-image-dot"></div><div id="loading-percentage"></div></div></div><script>const loadingPercentage=document.getElementById("loading-percentage");loadingPercentage.style.color="black";let loadingPercentageTimer=setInterval(function(){var e=document.querySelector(".pace-progress");e&&(e=e.getAttribute("data-progress-text"))!==loadingPercentage.textContent&&"60%"===(loadingPercentage.textContent=e)&&clearInterval(loadingPercentageTimer)},100);const preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",()=>{preloader.endLoading()})</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/favicon.webp" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">142</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">47</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(/img/cover/19.jpg)"><nav id="nav"><span id="blog-info"><a href="/" title="ZWN's blog"><span class="site-name">ZWN's blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">cs224w-图机器学习1</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-14T01:41:41.000Z" title="发表于 2024-08-14 09:41:41">2024-08-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-14T04:19:24.078Z" title="更新于 2024-08-14 12:19:24">2024-08-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0-%E8%AF%BE%E5%A4%96%E6%8B%93%E5%B1%95/">学习-课外拓展</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>16分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>cs224w-图机器学习1</h1><h2 id="Introduction">Introduction</h2><p>选择图的原因：图是用于描述并分析有关联 / 互动的实体的一种普适语言。它不将实体视为一系列孤立的点，而认为其互相之间有关系。它是一种很好的描述领域知识的方式。</p><h3 id="网络与图的分类">网络与图的分类</h3><ol><li><strong>networks / natural graphs</strong>：自然表示为图<ol><li>Social networks: Society is a collection of 7+ billion individuals</li><li>Communication and transactions: Electronic devices, phone calls, financial transactions</li><li>Biomedicine: Interactions between genes/proteins regulate life（大概是基因或蛋白质之间互动从而调节生理活动的过程）</li><li>Brain connections: Our thoughts are hidden in the connections between billions of neurons</li></ol></li><li><strong>graphs</strong>：作为一种表示方法<ol><li>Information/knowledge are organized and linked</li><li>Software can be represented as a graph</li><li>Similarity networks: Connect similar data points</li><li>Relational structures: Molecules, Scene graphs, 3D shapes, Particle-based physics simulations</li></ol></li><li>有时 network 和 graph 之间的差别是模糊的</li></ol><h3 id="传统深度学习与图神经网络">传统深度学习与图神经网络</h3><img src="image-20240814083218068.png" style="zoom:67%"><blockquote><p>这里相对图而言，文本、图片等甚至是更结构化的数据，它们都能在特定维度下进行规则的表示</p></blockquote><p>复杂领域会有丰富的关系结构，可以被表示为<strong>关系图</strong> relational graph，通过显式地建模关系，可以获得更好的表现但是现代深度学习工具常用于建模简单的序列 sequence（如文本、语音等具有线性结构的数据）和 grid（图片具有平移不变性，可以被表示为 fixed size grids 或 fixed size standards） 这些传统工具很难用于图的建模，其难点在于网络的复杂：</p><ul><li><p>任意的大小和复杂的拓扑结构（即，没有像图像那样的空间局部性）</p></li><li><p>没有基准点，没有节点固定的顺序。没有那种上下左右的方向</p></li><li><p>经常出现动态的图，而且会有多模态的特征</p></li></ul><p>图神经网络基本模型如下，我们输入一张图，输出预测的结果，所以关键点就在于如何设计中间的神经网络</p><img src="https://i-blog.csdnimg.cn/blog_migrate/09246c0a0303fb6eda1c887da66b8265.png" style="zoom:67%"><ol><li><p>监督机器学习全流程图：</p><p><img src="https://i-blog.csdnimg.cn/blog_migrate/0b0d63475770a7eba704ed40529064a0.png" style="zoom:67%">在传统机器学习流程中，我们需要对原始数据进行<strong>特征工程</strong> feature engineering（比如手动提取特征等)，但是现在我们使用<strong>表示学习</strong> representation learning 的方式来自动学习到数据的特征，直接应用于下游预测任务，这杨就相当于删除了功能工程步骤</p></li><li><p>图的表示学习：大致来说就是将原始的节点（或链接、或图）表示为向量（嵌入 <strong>embedding</strong>），图中相似的节点会被 embed 得靠近（指同一实体，在节点空间上相似，在向量空间上就也应当相似）</p><img src="https://i-blog.csdnimg.cn/blog_migrate/07e051e1e5cd81c0869a2e288da11bd3.png" style="zoom:67%"><p>我们可以想到的表征学习的方法是映射我们图的节点到d维嵌入，或者说到d维向量，这样看起来好像是网络中的节点紧密地嵌入到嵌入空间中。因此，目标是学习将要使用的函数f，将节点映射到这些d维实值向量中，这个向量会在这里称呼它为一个representation，或给定节点的特征表示或嵌入，再到整个图的嵌入，给定链接的嵌入，依此类推。</p></li><li><p>cs224w 本课程将聚焦图的机器学习和表示学习多个领域，课程大纲如下：</p><ol><li>Traditional methods: Graphlets, Graph Kernels</li><li>Methods for node embeddings: DeepWalk, Node2Vec</li><li>Graph Neural Networks: GCN, GraphSAGE, GAT, Theory of GNNs</li><li>Knowledge graphs and reasoning: TransE, BetaE</li><li>Deep generative models for graphs</li><li>Applications to Biomedicine, Science, Industry</li></ol></li></ol><h2 id="Applications-of-Graph-ML">Applications of Graph ML</h2><p>四类图机器学习任务</p><img src="https://i-blog.csdnimg.cn/blog_migrate/e6f6ccff3a5a534c93ace6510bbddfdc.png" style="zoom:50%"><ol><li>节点级别 node level</li><li>边级别 edge level</li><li>社区 / 子图级别 community(subgraph) level</li><li>图级别，包括预测任务 graph-level prediction 和 图生成任务 graph generation</li></ol><h3 id="各类型的典型任务">各类型的典型任务</h3><ul><li><p>节点分类：预测节点的属性</p><ul><li><strong>示例</strong>：对在线用户/物品进行分类</li></ul></li><li><p>链接预测：预测两个节点之间是否存在缺失的链接</p><ul><li><strong>示例</strong>：知识图谱完善</li></ul></li><li><p>图分类：对不同的图进行分类</p><ul><li><strong>示例</strong>：分子属性预测</li></ul></li><li><p>聚类：检测节点是否形成一个社区</p><ul><li><strong>示例</strong>：社交网络</li></ul></li><li><p>其他任务</p><ul><li>图生成：药物发现</li><li>图演化：物理模拟</li></ul></li></ul><h2 id="Choice-of-Graph-Representation">Choice of Graph Representation</h2><p>数据结构知识及其应用</p><h2 id="Traditional-Methods-for-ML-on-Graphs">Traditional Methods for ML on Graphs</h2><h3 id="章节前言">章节前言</h3><p>对于图机器学习，我们要调查的是不同级别的任务：我们可以考虑节点级别的预测任务，我们可以考虑链接级别或边缘级别的预测任务，比如考虑成对的节点并尝试预测该对节点是否已连接。我们可以考虑图形级预测，比如我们想要对整个图形进行预测。</p><img src="image-20240814104303822.png" style="zoom:50%"><p>传统机器学习 pipeline：设计并获取所有训练数据上节点 / 边 / 图的特征→训练机器学习模型→应用模型</p><p>图数据本身就会有特征/属性，但是我们还想获得说明其在网络中的位置、其局部网络结构 local network structure 之类的特征（这些额外的特征描述了网络的拓扑结构，能使预测更加准确） 。所以最终一共有两种特征：数据的 structural feature，以及其本身的 attributes and properities</p><img src="https://i-blog.csdnimg.cn/blog_migrate/5165e90599e3e83218e3b1061483bcb9.png" style="zoom:50%"><p>本章重点着眼于手工设计<strong>无向图</strong>三种数据层次上的特征（其 relational structure of the network），做预测问题</p><p>design choice</p><ol><li><strong>Features</strong>: d-dimensional vectors</li><li><strong>Objects</strong>: Nodes, edges, sets of nodes, entire graphs</li><li><strong>Objective function</strong>: What task are we aiming to solve?</li></ol><h3 id="Traditional-Feature-based-Methods-Node">Traditional Feature-based Methods: Node</h3><p>我们可以想到的方式是，我们将一个图作为一组顶点和边的集合，我们想学习一个函数，每一个节点都会给我们一个真实的预测，也即：给定 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo>=</mo><mo stretchy="false">(</mo><mi>V</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">G=(V,E)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal">G</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.22222em">V</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.05764em">E</span><span class="mclose">)</span></span></span></span> ，找到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>:</mo><mi>V</mi><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">f:V\to \mathbb R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">:</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68889em;vertical-align:0"></span><span class="mord mathbb">R</span></span></span></span>。问题是，我们如何学习该功能，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span> 会做出这些预测吗？</p><h4 id="特征抽取">特征抽取</h4><p>目标：找到能够描述节点在网络中结构与位置的特征，也就是说目标是表征给定节点周围的网络结构，以及某种意义上节点在更广泛的网络环境中的位置。我们将讨论四个不同的使我们能够做到这一点的方法。</p><img src="https://i-blog.csdnimg.cn/blog_migrate/9012a9284f3d3d62ec60f79065208b9a.png" style="zoom:50%"><h4 id="node-degree">node degree</h4><p>缺点在于仅考虑所有相邻节点，且将节点的所有邻居视为同等重要的</p><h4 id="node-centrality-c-v">node centrality <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">c_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span></h4><p>考虑了节点的重要性，几种衡量方法如下</p><ul><li><p><strong>eigenvector centrality</strong>：认为如果节点邻居重要，那么节点本身也重要</p><ul><li><p>因此节点 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03588em">v</span></span></span></span> 的 centrality 是邻居 centrality 的加总： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>v</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>λ</mi></mfrac><munder><mo>∑</mo><mrow><mi>u</mi><mo>∈</mo><mi>N</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow></munder><msub><mi>c</mi><mi>u</mi></msub></mrow><annotation encoding="application/x-tex">c_v=\frac{1}{\lambda}\sum\limits_{u\in N(v)}c_u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.0611129999999998em;vertical-align:-1.216005em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">λ</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.7500050000000001em"><span style="top:-2.058995em;margin-left:0"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:.10903em">N</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.0000050000000003em"><span class="pstrut" style="height:3em"></span><span><span class="mop op-symbol small-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.216005em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">u</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>（ <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span></span></span></span> 是某个正的常数）</p></li><li><p>这是个递归式，解法是将其转换为矩阵形式： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mi mathvariant="bold">c</mi><mo>=</mo><mrow><mi mathvariant="bold">A</mi><mi mathvariant="bold">c</mi></mrow></mrow><annotation encoding="application/x-tex">\lambda \mathbf{c}=\mathbf{Ac}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span><span class="mord mathbf">c</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68611em;vertical-align:0"></span><span class="mord"><span class="mord mathbf">Ac</span></span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68611em;vertical-align:0"></span><span class="mord mathbf">A</span></span></span></span> 是邻接矩阵，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">c</mi></mrow><annotation encoding="application/x-tex">\mathbf{c}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.44444em;vertical-align:0"></span><span class="mord mathbf">c</span></span></span></span> 是 centralty 向量。</p></li><li><p>从而发现 centrality 就是特征向量。根据 Perron-Frobenius Theorem<a href="#fn2">2</a> 可知最大的特征值 $ \lambda_{max}$​ 总为正且唯一，对应的 leading eigenvector <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">c</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{c}_{max}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.59444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathbf">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ma</span><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> ​就是 centrality 向量</p></li></ul></li><li><p><strong>betweenness centrality</strong>：认为如果一个节点处在很多节点对的最短路径上，那么这个节点是重要的。地处交通要道</p><ul><li><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>c</mi><mi>v</mi></msub><mo>=</mo><munder><mo>∑</mo><mrow><mi>s</mi><mo mathvariant="normal">≠</mo><mi>v</mi><mo mathvariant="normal">≠</mo><mi>t</mi></mrow></munder><mfrac><mrow><mi mathvariant="normal">#</mi><mo stretchy="false">(</mo><mi>s</mi><mtext>和</mtext><mi>t</mi><mtext>之间包含</mtext><mi>v</mi><mtext>的最短路径</mtext><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">#</mi><mo stretchy="false">(</mo><mi>s</mi><mtext>和</mtext><mi>t</mi><mtext>之间的最短路径</mtext><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">c_v=\sum\limits_{s\neq v\neq t}\frac{\#(s 和 t 之间包含 v 的最短路径)}{\#(s 和 t 之间的最短路径)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.865221em;vertical-align:-1.438221em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em"><span style="top:-1.8478869999999998em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mrel mtight"><span class="mrel mtight"><span class="mord vbox mtight"><span class="thinbox mtight"><span class="rlap mtight"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="inner"><span class="mord mtight"><span class="mrel mtight"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel mtight">=</span></span><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span><span class="mrel mtight"><span class="mrel mtight"><span class="mord vbox mtight"><span class="thinbox mtight"><span class="rlap mtight"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="inner"><span class="mord mtight"><span class="mrel mtight"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel mtight">=</span></span><span class="mord mathnormal mtight">t</span></span></span></span><span style="top:-3.0500049999999996em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.438221em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">#</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mord cjk_fallback">和</span><span class="mord mathnormal">t</span><span class="mord cjk_fallback">之间的最短路径</span><span class="mclose">)</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">#</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mord cjk_fallback">和</span><span class="mord mathnormal">t</span><span class="mord cjk_fallback">之间包含</span><span class="mord mathnormal" style="margin-right:.03588em">v</span><span class="mord cjk_fallback">的最短路径</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p></li><li><p><img src="https://i-blog.csdnimg.cn/blog_migrate/cdd4baad82a4e0abbbcf13986f070b56.png" style="zoom:67%"># 意思是：the number of…</p></li><li><p>图中这个 between 应该是写错了……</p></li></ul></li><li><p><strong>closeness centrality</strong>：认为如果一个节点距其他节点之间距离最短，那么认为这个节点是重要的</p><ul><li><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>c</mi><mi>v</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><munder><mo>∑</mo><mrow><mi>u</mi><mo mathvariant="normal">≠</mo><mi>v</mi></mrow></munder><mi>u</mi><mtext>和</mtext><mi>v</mi><mtext>之间的最短距离</mtext></mrow></mfrac></mrow><annotation encoding="application/x-tex">c_v=\frac{1}{\sum\limits_{u\neq v}u 和 v 之间的最短距离}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:3.145661em;vertical-align:-1.824221em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.7500050000000001em"><span style="top:-2.097887em;margin-left:0"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mrel mtight"><span class="mrel mtight"><span class="mord vbox mtight"><span class="thinbox mtight"><span class="rlap mtight"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="inner"><span class="mord mtight"><span class="mrel mtight"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel mtight">=</span></span><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span></span><span style="top:-3.0000050000000003em"><span class="pstrut" style="height:3em"></span><span><span class="mop op-symbol small-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.138221em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">u</span><span class="mord cjk_fallback">和</span><span class="mord mathnormal" style="margin-right:.03588em">v</span><span class="mord cjk_fallback">之间的最短距离</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.824221em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p></li><li><img src="https://i-blog.csdnimg.cn/blog_migrate/f18f854e75b20d9a67f1c3c27e1215d8.png" style="zoom:67%"></li></ul></li></ul><h4 id="clustering-coefficient"><strong>clustering coefficient</strong></h4><p>衡量节点邻居的连接程度 描述节点的局部结构信息</p><img src="https://i-blog.csdnimg.cn/blog_migrate/0b5343d5f049ba9fceb383be6d234388.png" style="zoom:67%"><p>这种$$\begin{pmatrix} k_{v} \ 2 \end{pmatrix}$$是组合数的写法，和国内常用的 C 写法上下是相反的。所以这个式子代表 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03588em">v</span></span></span></span> 邻居所构成的节点对，即潜在的连接数。整个公式衡量节点邻居的连接有多紧密。第 1 个例子： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>v</mi></msub><mo>=</mo><mn>6</mn><mi mathvariant="normal">/</mi><mn>6</mn></mrow><annotation encoding="application/x-tex">e_v=6/6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">6/6</span></span></span></span> 第 2 个例子： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>v</mi></msub><mo>=</mo><mn>3</mn><mi mathvariant="normal">/</mi><mn>6</mn></mrow><annotation encoding="application/x-tex">e_v=3/6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">3/6</span></span></span></span> 第 3 个例子： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>v</mi></msub><mo>=</mo><mn>0</mn><mi mathvariant="normal">/</mi><mn>6</mn></mrow><annotation encoding="application/x-tex">e_v=0/6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">0/6</span></span></span></span></p><img src="https://i-blog.csdnimg.cn/blog_migrate/4f0f127ef563ef0b8aef5e55a8a09d2e.png" style="zoom:67%"><h4 id="graphlets-有根连通异构子图"><strong>graphlets</strong> 有根连通异构子图</h4><p><img src="https://i-blog.csdnimg.cn/blog_migrate/a6ca5492cdd38d53f420a3af48c002c1.png" alt=""></p><p>对于某一给定节点数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03148em">k</span></span></span></span>，会有 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">n_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>​ 个连通的异构子图。 就是说，这些图首先是 connected 的，其次这些图有 k 个节点，第三它们异构。</p><p>图中标的数字代表根节点可选的位置。例如对于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">G_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.83333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>​，两个节点是等价的，所以只有一种 graphlet；对于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">G_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.83333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>​，根节点有在中间和在边上两种选择，上下两个边上的点是等价的，所以只有两种 graphlet。其他的类似。节点数为 2-5 情况下一共能产生如图所示 73 种 graphlet。</p><p>这 73 个 graphlet 的核心概念就是<strong>不同的形状，不同的位置</strong>。</p><blockquote><p>注意这里的 graphlet 概念和后文图的 graphlet kernel 的概念不太一样。具体的后文再讲</p></blockquote><p><strong>Graphlet Degree Vector (GDV)</strong>: Graphlet-base features for nodes GDV 与其他两种描述节点结构的特征的区别：</p><ul><li><p><strong>Degree</strong> counts #(edges) that a node touches</p></li><li><p><strong>Clustering coefficient</strong> counts #(triangles) that a node touches.</p></li><li><p><strong>GDV</strong> counts #(graphlets) that a node touches</p></li><li><p><strong>度</strong>：节点连接的边的数量</p></li><li><p><strong>聚类系数</strong>：节点连接的三角形的数量</p></li><li><p><strong>GDV</strong>：节点连接的图元的数量</p></li></ul><p><strong>Graphlet Degree Vector (GDV)</strong>: A count vector of graphslets rooted at a given node.一个以给定节点为根的图元计数向量。</p><img src="https://i-blog.csdnimg.cn/blog_migrate/4f878f0398e271a4a721d1844ea57580.png" style="zoom:50%"><p>如图所示，对四种 graphlet， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.03588em">v</span></span></span></span> 的每一种 graphlet 的数量作为向量的一个元素。</p><blockquote><p>注意：graphlet c 的情况不存在，是因为像 graphlet b 那样中间那条线连上了。这是因为 graphlet 是 induced subgraph，所以那个边也存在，所以 c 情况不存在。</p></blockquote><p>考虑 2-5 个节点的 graphlets，我们得到一个长度为 73 个坐标 coordinate（就前图所示一共 73 种 graphlet）的向量 GDV，描述该点的局部拓扑结构 topology of node’s neighborhood，可以捕获距离为 4 hops 的互联性 interconnectivities。</p><p>相比节点度数或 clustering coefficient，GDV 能够描述两个节点之间更详细的节点局部拓扑结构相似性 local topological similarity。</p><h4 id="Node-Level-Feature-Summary">Node Level Feature: Summary</h4><p>这些特征可以分为两类：</p><ol><li><p>Importance-based features: 捕获节点在图中的重要性</p><img src="https://i-blog.csdnimg.cn/blog_migrate/65cb2274336adaea37b71075ddf0c717.png" style="zoom:67%"></li><li><p>Structure-based features: 捕获节点附近的拓扑属性</p><img src="https://i-blog.csdnimg.cn/blog_migrate/d35a313598ba34d320e073c0ef880536.png" style="zoom:67%"></li></ol><h4 id="Discussion">Discussion</h4><img src="https://i-blog.csdnimg.cn/blog_migrate/85d0014cd91bc3049eaf022ab3eb78f3.png" style="zoom:67%"><p>就我的理解，大致来说，传统节点特征只能识别出结构上的相似，不能识别出图上空间、距离上的相似</p><h3 id="Traditional-Feature-based-Methods-Link">Traditional Feature-based Methods: Link</h3><ol><li><p>预测任务是基于已知的边，预测新链接的出现。测试模型时，将每一对无链接的点对进行排序，取存在链接概率最高的 K 个点对，作为预测结果</p><p><img src="https://i-blog.csdnimg.cn/blog_migrate/e40f3e47a6873e676fc416fa9ed0d122.png" alt=""></p></li><li><p>特征在点对上</p></li><li><p>有时你也可以直接将两个点的特征合并 concatenate 起来作为点对的特征，来训练模型。但这样做的缺点就在于失去了点之间关系的信息。</p></li><li><p>链接预测任务的两种类型：随机缺失边；随时间演化边<img src="https://i-blog.csdnimg.cn/blog_migrate/354bf33ba86b76876e78e167e4a5be2a.png" alt=""> 图中的 ’ 念 prime<br>第一种假设可以以蛋白质之间的交互作用举例，缺失的是研究者还没有发现的交互作用。（但这个假设其实有问题，因为研究者不是随机发现新链接的，新链接的发现会受到已发现链接的影响。在网络中有些部分被研究得更彻底，有些部分就几乎没有什么了解，不同部分的发现难度不同）<br>第二种假设可以以社交网络举例，随着时间流转，人们认识更多朋友。</p></li><li><p>基于相似性进行链接预测：计算两点间的相似性得分（如用共同邻居衡量相似性），然后将点对进行排序，得分最高的 n 组点对就是预测结果，与真实值作比<img src="https://i-blog.csdnimg.cn/blog_migrate/6703564c8c473f44ca339102eb2c5da5.png" alt=""></p></li><li><p><strong>distance-based feature</strong>：两点间最短路径的长度<img src="https://i-blog.csdnimg.cn/blog_migrate/b0c60db90cb5e27075db682178d9a97c.png" alt="">这种方式的问题在于没有考虑两个点邻居的重合度 the degree of neighborhood overlap，如 B-H 有 2 个共同邻居，B-E 和 A-B 都只有 1 个共同邻居。</p></li><li><p><strong>local neighborhood overlap</strong>：捕获节点的共同邻居数<img src="https://i-blog.csdnimg.cn/blog_migrate/73caa0598f1c9eebae3702b956ff127d.png" alt=""> common neighbors 的问题在于度数高的点对就会有更高的结果，Jaccard’s coefficient 是其归一化后的结果。<br>Adamic-Adar index 在实践中表现得好。在社交网络上表现好的原因：有一堆度数低的共同好友比有一堆名人共同好友的得分更高。</p></li><li><p><strong>global neighborhood overlap</strong><br>local neighborhood overlap 的限制在于，如果两个点没有共同邻居，值就为 0。<img src="https://i-blog.csdnimg.cn/blog_migrate/3304587dda329ad7bf10081e67366f0a.png" alt=""><br>但是这两个点未来仍有可能被连接起来。所以我们使用考虑全图的 global neighborhood overlap 来解决这一问题。<br><strong>Katz index</strong>：计算点对之间所有长度路径的条数<br>计算方式：邻接矩阵求幂</p><ol><li>邻接矩阵的 k 次幂结果，每个元素就是对应点对之间长度为 k 的路径的条数</li><li>证明：<img src="https://i-blog.csdnimg.cn/blog_migrate/57234fcca75a6110d09963cb9b5ad859.png" alt="">显然 A u v \mathbf{A}<em>{uv} Auv​代表 u 和 v 之间长度为 1 的路径的数量<br><img src="https://i-blog.csdnimg.cn/blog_migrate/a96525267af0624a7581e3cc78e79231.png" alt="">计算 u u u 和 v v v 之间长度为 2 的路径数量，就是计算每个 u u u 的邻居 A u i \mathbf{A}</em>{ui} Aui​ （与 u u u 有 1 条长度为 1 的路径）与 v v v 之间长度为 1 的路径数量 P i v (1) \mathbf{P}^{(1)}<em>{iv} Piv(1)​ 即 A i v \mathbf{A}</em>{iv} Aiv​ 的总和 ∑ i A u i ∗ A i v = A u v 2 \sum_i \mathbf{A}<em>{ui}*\mathbf{A}</em>{iv}=\mathbf{A}_{uv}^2 ∑i​Aui​∗Aiv​=Auv2​<br>同理，更高的幂（更远的距离）就重复过程，继续乘起来</li><li>从而得到 Katz index 的计算方式：<img src="https://i-blog.csdnimg.cn/blog_migrate/3382c85a1a5a5ee729d4efd4bc1479ae.png" alt=""><img src="https://i-blog.csdnimg.cn/blog_migrate/1c2ff13a548eee0b6212a5a6c964814e.png" alt="">discount factor β \beta β 会给比较长的距离以比较小的权重，exponentially with their length.<br>closed-form 闭式解，解析解 <a href="#fn8">8</a><br>解析解的推导方法我去查了，见尾注 <a href="#fn9">9</a></li></ol></li><li><p>Summary</p><ol><li>Distance-based features: Uses the shortest path length between two nodes but does not capture how neighborhood overlaps.</li><li>Local neighborhood overlap:<ol><li>Captures how many neighboring nodes are shared by two nodes.</li><li>Becomes zero when no neighbor nodes are shared.</li></ol></li><li>Global neighborhood overlap:<ol><li>Uses global graph structure to score two nodes.</li><li>Katz index counts #paths of all lengths between two nodes.</li></ol></li></ol></li></ol><h3 id="Traditional-Feature-based-Methods-Graph">Traditional Feature-based Methods: Graph</h3><ol><li><p>图级别特征构建目标：找到能够描述全图结构的特征</p></li><li><p>Background: Kernel Methods<img src="https://i-blog.csdnimg.cn/blog_migrate/405cd2b7f9d12103c95975610d4f199d.png" alt=""> 就是，核这一部分其实我一直都没搞懂，以前看 SVM 啥的时候就没好好学都是直接跳的，所以核本来是什么我也不知道……<br>b/w=between<br>off-the-shelf 现成的<br>不过单纯学习图机器学习的话只要按照图中所说原意来理解应该就行了：两个图的核 K ( G , G ‘ ) K(G,G^<code>) K(G,G‘) 以标量**衡量其相似度**，存在特征表示 ϕ (⋅) \phi (\cdot) ϕ(⋅) 使得 K ( G , G ‘ ) = ϕ (G) T ϕ ( G ‘ ) K(G,G^</code>)=\phi(G)^T\phi(G^`) K(G,G‘)=ϕ(G)Tϕ(G‘)<a href="#fn10">10</a>，定义好核后就可以直接应用核 SVM 之类的传统机器学习模型。<br>这个 ϕ \phi ϕ 是个表示向量，可能不需要被显式地计算出来<br>半正定矩阵特征值非负的证明开我之前写的博文：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/PolarisRisingWar/article/details/115598815">从 0 开始的 GNN 导学课程笔记</a></p></li><li><p>Overview<img src="https://i-blog.csdnimg.cn/blog_migrate/11f5aa1db10e9142935739976563e491.png" alt=""></p></li><li><p>graph kernel: key idea<img src="https://i-blog.csdnimg.cn/blog_migrate/42a83884362f6bd4a55832742552a440.png" alt="">bag-of-words 相当于是把文档表示成一个向量，每个元素代表对应 word 出现的次数。<br>此处讲述的特征抽取方法也将是 bag-of-something 的形式，将图表示成一个向量，每个元素代表对应 something 出现的次数（这个 something 可以是 node, degree, graphlet, color）</p><p>光用 node 不够的话，可以设置一个 degree kernel，用 bag-of-degrees 来描述图特征<img src="https://i-blog.csdnimg.cn/blog_migrate/9357a3330a874cf2e8a49084b7651213.png" alt=""></p></li><li><p>graphlet features</p><ol><li>Key idea: Count the number of different graphlets in a graph.</li><li>注意这里对 graphlet 的定义跟上文节点层面特征抽取里的 graphlet 不一样。区别在于：<ol><li>Nodes in graphlets here do not need to be connected (allows for isolated nodes)</li><li>The graphlets here are not rooted.</li></ol></li><li>对每一种节点数，可选的 graphlet：<img src="https://i-blog.csdnimg.cn/blog_migrate/030ec6d3c117bb13018f06bb53758820.png" alt=""></li><li><strong>graphlet count vector</strong>：每个元素是图中对应 graphlet 的数量<img src="https://i-blog.csdnimg.cn/blog_migrate/9b8cfb3e79a00219ef10cc8537d0a1a7.png" alt=""><img src="https://i-blog.csdnimg.cn/blog_migrate/f259c1f93ede4a3de91fff6d3c3b51c3.png" alt=""></li><li>graphlet kernel 就是直接点积两个图的 graphlet count vector 得到相似性。对于图尺寸相差较大的情况需进行归一化<img src="https://i-blog.csdnimg.cn/blog_migrate/88b019e0bb958a9e747da95e8f35abf4.png" alt=""> skew 扭曲<br>h 捕获了图中我们要的 graphlet 的 frequency 或 proportion</li><li>graphlet kernel 的限制：计算昂贵（这一部分的知识对我来说超纲了，我就只知道有这么回事就完了，我来不及学为啥了）<img src="https://i-blog.csdnimg.cn/blog_migrate/38b5c6e9c8778bae554844fad1dd3cdc.png" alt=""></li></ol></li><li><p><strong>Weisfeiler-Lehman Kernel</strong>：相比 graphlet kernel 代价较小，效率更高。<br>用节点邻居结构迭代地来扩充节点信息（vocabulary 在此仅作引申义？）<br><img src="https://i-blog.csdnimg.cn/blog_migrate/f70ade61ddb30136504e224dd75b3f43.png" alt=""></p><ol><li>实现算法：Weisfeiler-Lehman graph isomorphism test=color refinement<a href="#fn11">11</a><img src="https://i-blog.csdnimg.cn/blog_migrate/0c25d7c28d96f2fde450d3252d772188.png" alt=""> c v (k) c^{(k)}_v cv(k)​ 念成 c capital k of v</li><li>color refinement 示例<br>把邻居颜色聚集起来<img src="https://i-blog.csdnimg.cn/blog_migrate/9ab9685911c51475c50c5540da8442be.png" alt=""><br>对聚集后颜色取哈希值<img src="https://i-blog.csdnimg.cn/blog_migrate/e0981cdf437002a86e22b6a6566c211f.png" alt=""><br>把邻居颜色聚集起来<img src="https://i-blog.csdnimg.cn/blog_migrate/34af52c942b1fbd90f7e71bfdf539b80.png" alt=""><br>对聚集后颜色取哈希值<img src="https://i-blog.csdnimg.cn/blog_migrate/90c8a1104af5a7960828c39fb5600139.png" alt=""></li><li>进行 K 次迭代 <a href="#fn12">12</a> 后，用整个迭代过程中颜色出现的次数作为 Weisfeiler-Lehman graph feature<img src="https://i-blog.csdnimg.cn/blog_migrate/92503e6ed75bf7fdc13c0b42b66187ff.png" alt=""> 第一个图的特征应该是算错了，最后 3 个元素应该是 2 1 0</li><li>用上图的向量点积计算相似性，得到 WL kernel<img src="https://i-blog.csdnimg.cn/blog_migrate/2f4b098a06f3e8be6717a2d0df8d8140.png" alt=""></li><li>WL kernel 的优势在于计算成本低<img src="https://i-blog.csdnimg.cn/blog_migrate/8e4e23cf8f5ed291d75516c7a773673c.png" alt=""> w.r.t: with respect to<br>颜色个数最多是节点的个数：每一次就最多这么多个点上有颜色……</li></ol></li><li><p>Summary<img src="https://i-blog.csdnimg.cn/blog_migrate/cef8e17362eb89c123e85574c95c18eb.png" alt=""> 这个 color refinement 方法与 GNN 的相似性我认为有二，一在都聚集了节点邻居信息 <a href="#fn13">13</a>，GNN 详情见我撰写的后续课程笔记（就后面好几节课都讲了 GNN）；二在在 Lecture 9 中会讲的 GIN<a href="#fn14">14</a>。</p></li></ol><p>关于这个 GNN 空间方法为什么是聚集邻居信息啊，我主要看到过两种说法，一种是它反正就是这么干的，这么干本来就很符合直觉嘛（马克思说过，人是社会性的动物，节点受其邻居影响是很直觉的嘛，就像 KNN 一样）；另一种是其做法发源自其他方法，一说是来源于谱方法（但是具体怎么来的我没搞懂，反正就是好像经过一番推导可以从谱方法简化到空间方法），一说是受 belief propagation 启发，一说是受 CNN 启发。</p><h2 id="References">References</h2><blockquote><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/PolarisRisingWar/article/details/117287320">https://blog.csdn.net/PolarisRisingWar/article/details/117287320</a></p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://web.stanford.edu/class/cs224w/index.html">https://web.stanford.edu/class/cs224w/index.html</a></p></blockquote></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://zwn2001.space">琉璃月</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zwn2001.space/posts/cs224w-%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/">https://zwn2001.space/posts/cs224w-%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zwn2001.space" target="_blank">ZWN's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">图机器学习</a></div><div class="post_share"><div class="social-share" data-image="/img/cover/19.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://unpkg.com/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://unpkg.com/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/posts/%E8%AF%BBpaper3-Katana-Dual-Slicing-Based-Context-for-Learning-Bug-Fixes/" title="读paper3-Katana_Dual_Slicing-Based_Context_for_Learning_Bug_Fixes"><img class="cover" src="/img/cover/12.jpg" onerror='onerror=null,src="/img/404.webp"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">读paper3-Katana_Dual_Slicing-Based_Context_for_Learning_Bug_Fixes</div></div></a></div></nav><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/favicon.webp" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">琉璃月</div><div class="author-info__description">我虽无意逐鹿，却知苍生苦楚</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">142</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">47</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/ZWN2001"><i class="fab fa-github"></i><span>我的Github</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ZWN2001" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="fab fa-github"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">新域名：www.zwn2001.space，有效期：10年。https://www.zwn-blog.xyz已过期。访问时建议科学上网，否则博客内公式渲染会出现问题且速度慢。Ctrl+shift+r可强制刷新网站以避免浏览器缓存造成的更新不及时</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">cs224w-图机器学习1</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">1.1.</span> <span class="toc-text">Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%9B%BE%E7%9A%84%E5%88%86%E7%B1%BB"><span class="toc-number">1.1.1.</span> <span class="toc-text">网络与图的分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.2.</span> <span class="toc-text">传统深度学习与图神经网络</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Applications-of-Graph-ML"><span class="toc-number">1.2.</span> <span class="toc-text">Applications of Graph ML</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%84%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%85%B8%E5%9E%8B%E4%BB%BB%E5%8A%A1"><span class="toc-number">1.2.1.</span> <span class="toc-text">各类型的典型任务</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Choice-of-Graph-Representation"><span class="toc-number">1.3.</span> <span class="toc-text">Choice of Graph Representation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Traditional-Methods-for-ML-on-Graphs"><span class="toc-number">1.4.</span> <span class="toc-text">Traditional Methods for ML on Graphs</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AB%A0%E8%8A%82%E5%89%8D%E8%A8%80"><span class="toc-number">1.4.1.</span> <span class="toc-text">章节前言</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Traditional-Feature-based-Methods-Node"><span class="toc-number">1.4.2.</span> <span class="toc-text">Traditional Feature-based Methods: Node</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">特征抽取</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#node-degree"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">node degree</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#node-centrality-c-v"><span class="toc-number">1.4.2.3.</span> <span class="toc-text">node centrality cvc_vcv​</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#clustering-coefficient"><span class="toc-number">1.4.2.4.</span> <span class="toc-text">clustering coefficient</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#graphlets-%E6%9C%89%E6%A0%B9%E8%BF%9E%E9%80%9A%E5%BC%82%E6%9E%84%E5%AD%90%E5%9B%BE"><span class="toc-number">1.4.2.5.</span> <span class="toc-text">graphlets 有根连通异构子图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Node-Level-Feature-Summary"><span class="toc-number">1.4.2.6.</span> <span class="toc-text">Node Level Feature: Summary</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Discussion"><span class="toc-number">1.4.2.7.</span> <span class="toc-text">Discussion</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Traditional-Feature-based-Methods-Link"><span class="toc-number">1.4.3.</span> <span class="toc-text">Traditional Feature-based Methods: Link</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Traditional-Feature-based-Methods-Graph"><span class="toc-number">1.4.4.</span> <span class="toc-text">Traditional Feature-based Methods: Graph</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-number">1.5.</span> <span class="toc-text">References</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/cs224w-%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/" title="cs224w-图机器学习1"><img src="/img/cover/19.jpg" onerror='this.onerror=null,this.src="/img/404.webp"' alt="cs224w-图机器学习1"></a><div class="content"><a class="title" href="/posts/cs224w-%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/" title="cs224w-图机器学习1">cs224w-图机器学习1</a><time datetime="2024-08-14T01:41:41.000Z" title="发表于 2024-08-14 09:41:41">2024-08-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/%E8%AF%BBpaper3-Katana-Dual-Slicing-Based-Context-for-Learning-Bug-Fixes/" title="读paper3-Katana_Dual_Slicing-Based_Context_for_Learning_Bug_Fixes"><img src="/img/cover/12.jpg" onerror='this.onerror=null,this.src="/img/404.webp"' alt="读paper3-Katana_Dual_Slicing-Based_Context_for_Learning_Bug_Fixes"></a><div class="content"><a class="title" href="/posts/%E8%AF%BBpaper3-Katana-Dual-Slicing-Based-Context-for-Learning-Bug-Fixes/" title="读paper3-Katana_Dual_Slicing-Based_Context_for_Learning_Bug_Fixes">读paper3-Katana_Dual_Slicing-Based_Context_for_Learning_Bug_Fixes</a><time datetime="2024-08-11T12:30:40.000Z" title="发表于 2024-08-11 20:30:40">2024-08-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/%E8%AF%BBpaper2-%E5%9F%BA%E4%BA%8ELLM%E7%9A%84%E7%BC%BA%E9%99%B7%E4%BF%AE%E5%A4%8D/" title="读paper2-基于LLM的缺陷修复"><img src="/img/cover/20.jpg" onerror='this.onerror=null,this.src="/img/404.webp"' alt="读paper2-基于LLM的缺陷修复"></a><div class="content"><a class="title" href="/posts/%E8%AF%BBpaper2-%E5%9F%BA%E4%BA%8ELLM%E7%9A%84%E7%BC%BA%E9%99%B7%E4%BF%AE%E5%A4%8D/" title="读paper2-基于LLM的缺陷修复">读paper2-基于LLM的缺陷修复</a><time datetime="2024-08-09T11:12:45.000Z" title="发表于 2024-08-09 19:12:45">2024-08-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-LogSumExp%E6%8A%80%E5%B7%A7/" title="深度学习-LogSumExp技巧"><img src="/img/cover/30.jpg" onerror='this.onerror=null,this.src="/img/404.webp"' alt="深度学习-LogSumExp技巧"></a><div class="content"><a class="title" href="/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-LogSumExp%E6%8A%80%E5%B7%A7/" title="深度学习-LogSumExp技巧">深度学习-LogSumExp技巧</a><time datetime="2024-07-27T04:15:17.000Z" title="发表于 2024-07-27 12:15:17">2024-07-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/jupyter%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/" title="jupyter踩坑记录"><img src="/img/cover/28.jpg" onerror='this.onerror=null,this.src="/img/404.webp"' alt="jupyter踩坑记录"></a><div class="content"><a class="title" href="/posts/jupyter%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/" title="jupyter踩坑记录">jupyter踩坑记录</a><time datetime="2024-07-27T02:20:19.000Z" title="发表于 2024-07-27 10:20:19">2024-07-27</time></div></div></div></div></div></div></main><footer id="footer" style="background:0 0"></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span> 数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div><div class="js-pjax" id="rightMenu"><div class="rightMenu-group rightMenu-small"><a class="rightMenu-item" href="javascript:window.history.back();" rel="external nofollow noreferrer"><i class="fa fa-arrow-left"></i></a><a class="rightMenu-item" href="javascript:window.history.forward();" rel="external nofollow noreferrer"><i class="fa fa-arrow-right"></i></a><a class="rightMenu-item" href="javascript:window.location.reload();" rel="external nofollow noreferrer"><i class="fa fa-refresh"></i></a><a class="rightMenu-item" href="javascript:rmf.scrollToTop();" rel="external nofollow noreferrer"><i class="fa fa-arrow-up"></i></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-text"><a class="rightMenu-item" href="javascript:rmf.copySelect();" rel="external nofollow noreferrer"><i class="fa fa-copy"></i><span>复制</span></a><a class="rightMenu-item" href="javascript:window.open(&quot;https://www.google.com/search?q=&quot;+window.getSelection().toString());" rel="external nofollow noreferrer"><i class="iconfont icon-baidu"></i><span>搜索</span></a><a class="rightMenu-item" href="javascript:rmf.searchinThisPage();" rel="external nofollow noreferrer"><i class="fas fa-search"></i><span>站内搜索</span></a><a class="rightMenu-item" href="#post-comment" onclick="rmf.yinyong()"><i class="fa-solid fa-message"></i><span>引用文本评论</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-too"><a class="rightMenu-item" href="javascript:window.open(window.getSelection().toString());window.location.reload();" rel="external nofollow noreferrer"><i class="fa fa-link"></i><span>转到链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-paste"><a class="rightMenu-item" href="javascript:rmf.paste()" rel="external nofollow noreferrer"><i class="fa fa-copy"></i><span>粘贴</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-to"><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()" rel="external nofollow noreferrer"><i class="fa fa-window-restore"></i><span>新窗口打开</span></a><a class="rightMenu-item" id="menu-too" href="javascript:rmf.open()" rel="external nofollow noreferrer"><i class="fa fa-link"></i><span>转到链接</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()" rel="external nofollow noreferrer"><i class="fa fa-copy"></i><span>复制链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-img"><a class="rightMenu-item" href="javascript:rmf.saveAs()" rel="external nofollow noreferrer"><i class="fa fa-download"></i><span>保存图片</span></a><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()" rel="external nofollow noreferrer"><i class="fa fa-window-restore"></i><span>在新窗口打开</span></a><a class="rightMenu-item" href="javascript:rmf.click()" rel="external nofollow noreferrer"><i class="fa fa-arrows-alt"></i><span>全屏显示</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()" rel="external nofollow noreferrer"><i class="fa fa-copy"></i><span>复制图片链接</span></a></div><div class="rightMenu-group rightMenu-line"><a class="rightMenu-item" href="javascript:rmf.switchDarkMode();" rel="external nofollow noreferrer"><i class="fa fa-moon"></i><span>昼夜切换</span></a><a class="rightMenu-item" href="javascript:rmf.translate();" rel="external nofollow noreferrer"><i class="iconfont icon-fanti"></i><span>繁简转换</span></a><a class="rightMenu-item" href="javascript:rmf.switchReadMode();" rel="external nofollow noreferrer"><i class="fa fa-book"></i><span>阅读模式</span></a><a class="rightMenu-item" href="javascript:fullScreen();" rel="external nofollow noreferrer"><i class="fas fa-expand"></i><span>进入全屏</span></a></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://unpkg.com/@fancyapps/ui/dist/fancybox/fancybox.umd.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://unpkg.com/pangu/dist/browser/pangu.min.js").then(()=>{pangu.autoSpacingPage()})}function panguInit(){GLOBAL_CONFIG_SITE.isPost&&panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://unpkg.com/katex/dist/katex.min.css"><script src="https://unpkg.com/katex/dist/contrib/copy-tex.min.js"></script><script>document.querySelectorAll("#article-container span.katex-display").forEach(a=>{btf.wrap(a,"div",{class:"katex-wrap"})})</script><script>function getGiscusTheme(e){return"dark"===e?"dark":"light"}function loadGiscus(){var e,t=Object.assign({src:"https://giscus.app/client.js","data-repo":"ZWN2001/ZWN2001.github.io","data-repo-id":"R_kgDOGH1XWg","data-category-id":"DIC_kwDOGH1XWs4CXnHJ","data-mapping":"pathname","data-theme":getGiscusTheme(document.documentElement.getAttribute("data-theme")),"data-reactions-enabled":"1",crossorigin:"anonymous",async:!0},{"data-lang":"zh-CN","data-loading":"lazy",crossorigin:"anonymous","data-mapping":"og:title","data-input-position":"top","data-category":"Announcements"}),a=document.createElement("script");for(e in t)a.setAttribute(e,t[e]);document.getElementById("giscus-wrap").insertAdjacentElement("afterbegin",a)}function changeGiscusTheme(e){var t;e={setConfig:{theme:getGiscusTheme(e)}},(t=document.querySelector("iframe.giscus-frame"))&&t.contentWindow.postMessage({giscus:e},"https://giscus.app")}function loadOtherComment(){loadGiscus()}btf.addModeChange("giscus",changeGiscusTheme),btf.loadComment(document.getElementById("giscus-wrap"),loadGiscus)</script></div><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script><script type="text/javascript" src="/js/rightmenu.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span> 数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script data-pjax>var parent,child;document.getElementById("recent-posts")&&"/"===location.pathname&&(parent=document.getElementById("recent-posts"),child='<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="https://zwn2001.space/categories/编程知识/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 琉璃月の编程知识 (13)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://zwn2001.space/categories/实用知识/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">💡 琉璃月の实用知识 (12)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://zwn2001.space/categories/学习-课外拓展/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">👩‍💻 琉璃月の学习-课外拓展 (30)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://zwn2001.space/categories/学习-课内知识/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📒 琉璃月の学习-课内知识 (55)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="https://zwn2001.space/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>',console.log("已挂载magnet"),parent.insertAdjacentHTML("afterbegin",child))</script><style>#catalog_magnet{flex-wrap:wrap;display:flex;width:100%;justify-content:space-between;padding:10px 10px 0 10px;align-content:flex-start}.magnet_item{flex-basis:calc(50% - 5px);background:#f2f2f2;margin-bottom:10px;border-radius:8px;transition:all .2s ease-in-out}.magnet_item:hover{background:#b30070}.magnet_link_more{color:#555}.magnet_link{color:#000}.magnet_link:hover{color:#fff}@media screen and (max-width:600px){.magnet_item{flex-basis:100%}}.magnet_link_context{display:flex;padding:10px;font-size:16px;transition:all .2s ease-in-out}.magnet_link_context:hover{padding:10px 20px}</style><style></style></body></html>