<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>默认参数的最优化 | ZWN's blog</title><meta name="author" content="琉璃月"><meta name="copyright" content="琉璃月"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="默认参数的最优化  场景：速读时，有些学生速读 1 次就能复述，有些需要 2 次，有些需要 N 次，而建议速读次数 X 是个默认值，这个默认值到底取多少合适 技术：使用线性回归技术  TODO1：非常全的总结：https:&#x2F;&#x2F;www.cnblogs.com&#x2F;chuqianyu&#x2F;p&#x2F;17684614.html 网格搜索(Grid Search) 网格搜索是暴力搜索，在给定超参搜索空间内，尝试所有超参"><meta property="og:type" content="article"><meta property="og:title" content="默认参数的最优化"><meta property="og:url" content="https://zwn2001.space/posts/Graduate-Works/Course-Notes/%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0%E7%9A%84%E6%9C%80%E4%BC%98%E5%8C%96/index.html"><meta property="og:site_name" content="ZWN&#39;s blog"><meta property="og:description" content="默认参数的最优化  场景：速读时，有些学生速读 1 次就能复述，有些需要 2 次，有些需要 N 次，而建议速读次数 X 是个默认值，这个默认值到底取多少合适 技术：使用线性回归技术  TODO1：非常全的总结：https:&#x2F;&#x2F;www.cnblogs.com&#x2F;chuqianyu&#x2F;p&#x2F;17684614.html 网格搜索(Grid Search) 网格搜索是暴力搜索，在给定超参搜索空间内，尝试所有超参"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://zwn2001.space/img/cover/33.jpg"><meta property="article:published_time" content="2024-10-30T02:57:03.000Z"><meta property="article:modified_time" content="2024-10-30T07:12:09.875Z"><meta property="article:author" content="琉璃月"><meta property="article:tag" content="学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://zwn2001.space/img/cover/33.jpg"><link rel="shortcut icon" href="/img/favicon.webp"><link rel="canonical" href="https://zwn2001.space/posts/Graduate-Works/Course-Notes/%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0%E7%9A%84%E6%9C%80%E4%BC%98%E5%8C%96/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://unpkg.com/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://unpkg.com/@fancyapps/ui/dist/fancybox/fancybox.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!0,top_n_per_article:-1,unescape:!1,languages:{hits_empty:"找不到您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"简"},noticeOutdate:{limitDay:200,position:"top",messagePrev:"距离上次更新已经过去",messageNext:"天啦！注意内容可能过时。"},highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:300},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,source:{justifiedGallery:{js:"https://unpkg.com/flickr-justified-gallery/dist/fjGallery.min.js",css:"https://unpkg.com/flickr-justified-gallery/dist/fjGallery.css"}},isPhotoFigcaption:!0,islazyload:!1,isAnchor:!0,percent:{toc:!0,rightside:!0},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"默认参数的最优化",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2024-10-30 15:12:09"}</script><noscript><style type="text/css">#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,a){0!==a&&(a=864e5*a,t={value:t,expiry:(new Date).getTime()+a},localStorage.setItem(e,JSON.stringify(t)))},get:function(e){var t=localStorage.getItem(e);if(t){t=JSON.parse(t);if(!((new Date).getTime()>t.expiry))return t.value;localStorage.removeItem(e)}}},e.getScript=o=>new Promise((t,e)=>{const a=document.createElement("script");a.src=o,a.async=!0,a.onerror=e,a.onload=a.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)}),e.getCSS=(o,n=!1)=>new Promise((t,e)=>{const a=document.createElement("link");a.rel="stylesheet",a.href=o,n&&(a.id=n),a.onerror=e,a.onload=a.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","ffffff")};e=saveToLocal.get("theme"),"dark"===e?activateDarkMode():"light"===e&&activateLightMode(),e=saveToLocal.get("aside-status");void 0!==e&&("hide"===e?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/transpancy.css"><link rel="stylesheet" href="/css/iconfont.css"><link rel="stylesheet" href="/css/rightmenu.css"><link rel="stylesheet" href="/css/loadimg.css"><link rel="stylesheet" href="/css/project.css"><link type="text/html" rel="stylesheet" href="/css/wide_screen.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"><style>#recent-posts>.recent-post-item>.recent-post-info>.article-meta-wrap>.tags:before{content:"\A";white-space:pre}#recent-posts>.recent-post-item>.recent-post-info>.article-meta-wrap>.tags>.article-meta__separator{display:none}</style><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" src="/img/favicon.webp"><div class="loading-image-dot"></div><div id="loading-percentage"></div></div></div><script>const loadingPercentage=document.getElementById("loading-percentage");loadingPercentage.style.color="black";let loadingPercentageTimer=setInterval(function(){var e=document.querySelector(".pace-progress");e&&(e=e.getAttribute("data-progress-text"))!==loadingPercentage.textContent&&"60%"===(loadingPercentage.textContent=e)&&clearInterval(loadingPercentageTimer)},100);const preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",()=>{preloader.endLoading()})</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/favicon.webp" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">158</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">49</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(/img/cover/33.jpg)"><nav id="nav"><span id="blog-info"><a href="/" title="ZWN's blog"><span class="site-name">ZWN's blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">默认参数的最优化</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-10-30T02:57:03.000Z" title="发表于 2024-10-30 10:57:03">2024-10-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-10-30T07:12:09.875Z" title="更新于 2024-10-30 15:12:09">2024-10-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0-%E8%AF%BE%E5%A4%96%E6%8B%93%E5%B1%95/">学习-课外拓展</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>9分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>默认参数的最优化</h1><blockquote><p>场景：速读时，有些学生速读 1 次就能复述，有些需要 2 次，有些需要 N 次，而建议速读次数 X 是个默认值，这个默认值到底取多少合适<br>技术：使用线性回归技术</p></blockquote><p>TODO1：非常全的总结：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.cnblogs.com/chuqianyu/p/17684614.html">https://www.cnblogs.com/chuqianyu/p/17684614.html</a></p><h2 id="网格搜索-Grid-Search"><strong>网格搜索(Grid Search)</strong></h2><p>网格搜索是暴力搜索，在给定超参搜索空间内，尝试所有超参组合，最后搜索出最优的超参组合。sklearn已实现该方法，使用样例如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm, datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入数据</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"><span class="comment"># 定义超参搜索空间</span></span><br><span class="line">parameters = &#123;<span class="string">&#x27;kernel&#x27;</span>:(<span class="string">&#x27;linear&#x27;</span>, <span class="string">&#x27;rbf&#x27;</span>), <span class="string">&#x27;C&#x27;</span>:[<span class="number">1</span>, <span class="number">10</span>]&#125;</span><br><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line">svc = svm.SVC()</span><br><span class="line"><span class="comment"># 网格搜索</span></span><br><span class="line">clf = GridSearchCV(estimator = svc,</span><br><span class="line">                   param_grid = parameters,</span><br><span class="line">                   scoring = <span class="string">&#x27;accuracy&#x27;</span>,</span><br><span class="line">                   n_jobs = -<span class="number">1</span>,</span><br><span class="line">                   cv = <span class="number">5</span>)</span><br><span class="line">clf.fit(iris.data, iris.target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;详细结果:\n&#x27;</span>, pd.DataFrame.from_dict(clf.cv_results_))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最佳分类器:\n&#x27;</span>, clf.best_estimator_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最佳分数:\n&#x27;</span>, clf.best_score_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最佳参数:\n&#x27;</span>, clf.best_params_).</span><br></pre></td></tr></table></figure><p><code>sklearn.model_selection.GridSearchCV</code>的重要参数说明:</p><ul><li>estimator: scikit-learn模型。</li><li>param_grid: 超参搜索空间，即超参数字典。</li><li>scoring: 在交叉验证中使用的评估策略。</li><li>n_jobs: 并行任务数，-1为使用所有CPU。</li><li>cv: 决定采用几折交叉验证。</li></ul><h2 id="随机搜索-Randomized-Search"><strong>随机搜索(Randomized Search)</strong></h2><p>随机搜索是在搜索空间中采样出超参组合，然后选出采样组合中最优的超参组合。随机搜索的好处如下图所示：</p><p><img src="3.png" alt=""></p><p>解释图1，如果目前我们要搜索两个参数，但参数A重要而另一个参数B并没有想象中重要，网格搜索9个参数组合(A, B)，而由于模型更依赖于重要参数A，所以只有3个参数值是真正参与到最优参数的搜索工作中。反观随机搜索，随机采样9种超参组合，在重要参数A上会有9个参数值参与到搜索工作中，所以，<strong>在某些参数对模型影响较小时，使用随机搜索能让我们有更多的探索空间</strong>。</p><p>同样地，sklearn实现了随机搜索，样例代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm, datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> uniform</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入数据</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"><span class="comment"># 定义超参搜索空间</span></span><br><span class="line">distributions = &#123;<span class="string">&#x27;kernel&#x27;</span>:[<span class="string">&#x27;linear&#x27;</span>, <span class="string">&#x27;rbf&#x27;</span>], <span class="string">&#x27;C&#x27;</span>:uniform(loc=<span class="number">1</span>, scale=<span class="number">9</span>)&#125;</span><br><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line">svc = svm.SVC()</span><br><span class="line"><span class="comment"># 网格搜索</span></span><br><span class="line">clf = RandomizedSearchCV(estimator = svc,</span><br><span class="line">                         param_distributions = distributions,</span><br><span class="line">                         n_iter = <span class="number">4</span>,</span><br><span class="line">                         scoring = <span class="string">&#x27;accuracy&#x27;</span>,</span><br><span class="line">                         cv = <span class="number">5</span>,</span><br><span class="line">                         n_jobs = -<span class="number">1</span>,</span><br><span class="line">                         random_state = <span class="number">2021</span>)</span><br><span class="line">clf.fit(iris.data, iris.target)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;详细结果:\n&#x27;</span>, pd.DataFrame.from_dict(clf.cv_results_))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最佳分类器:\n&#x27;</span>, clf.best_estimator_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最佳分数:\n&#x27;</span>, clf.best_score_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最佳参数:\n&#x27;</span>, clf.best_params_)</span><br></pre></td></tr></table></figure><p>相比于网格搜索，sklearn随机搜索中主要改变的参数是param_distributions，负责提供超参值分布范围。</p><h2 id="贝叶斯优化-Bayesian-Optimization"><strong>贝叶斯优化(Bayesian Optimization)</strong></h2><p>调优的目的是要找到一组最优的超参组合，能使目标函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span> 达到全局最小值。</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.1600em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>x</mi><mo>∗</mo></msup><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>i</mi><mi>n</mi><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{matrix}x^*=argminf(x),x\in X\end{matrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2000000000000002em;vertical-align:-.35000000000000003em"></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8500000000000001em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.688696em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:.02778em">r</span><span class="mord mathnormal" style="margin-right:.03588em">g</span><span class="mord mathnormal">min</span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000000000000003em"><span></span></span></span></span></span></span></span></span></span></span></span></p><p>在机器学习中，<strong>目标函数</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span> 常是被称作<strong>expensive blackbox function</strong>，计算开销大且不一定为凸函数。为此，贝叶斯优化出现了，它特别适合针对expensive blackbox function找到全局最优</p><p>假设我们的真实的目标函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> 长下图这样：</p><p><img src="640.png" alt=""></p><p>注意: 目标函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> 的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> 是指超参数，我们希望找到最优的超参 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> 去得到最小的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> 。为什么用虚线表示 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> 呢？因为它是黑箱函数 (blackbox function)。也就是说我们根本不知道 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> 具体是什么样的，就好比我们不知道用某一个学习率的值训练出的模型效果会是什么样子。</p><p>这里有一个问题，每次尝试一种超参值 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> ，计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> 的代价是昂贵的，为了减轻开销，贝叶斯优化采用了<strong>代理模型 (surrogate model)</strong>，代理模型可以被看作是一个简单模型去拟合原本复杂且不好理解的模型。</p><p>贝叶斯优化使用了<strong>高斯过程 (gasussian processes, GP)</strong> 去构建代理模型，高斯过程的细节这里暂时不讲，有兴趣的小伙伴可以自行查阅资料了解。基于给定的输入和输出，GP 会推断出一个模型 (这里为代理模型)。假设我们从昂贵的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> 采样了 4 个点，然后我们把这 4 个点交给 GP，它会返回一个代理模型，如下图所示：</p><p><img src="2.png" alt=""></p><p><strong>绿色实线就是 GP 猜的代理模型，绿色条带是输出分布的标准差 (即为 Uncertainty)</strong>。我们有了代理模型，后续我们去找下一个合适的超参值，就能带入到计算开销相对较小的代理模型中，评估给定超参值的情况。</p><p>现在，我们来思考回之前提到的问题:&quot; <strong>如何找到下一个合适的点?</strong>&quot;，这个问题本质是在问：“哪里有全局最小的点？”，为了解决这个问题，我们要关注两个地方:</p><ul><li><p><strong>已开发区域</strong>: 在绿色实线上最低的超参点。因为很可能它附近存在全局最优点。</p></li><li><p><strong>未探索区域</strong>: 绿色实线上还未被探索的区域。比如上图，相比于 0.15-0.25 区间，0.65-0.75 区间更具有探索价值 (即该区间 Uncertainty 更大)。探索该区域有利于减少我们猜测的方差。</p></li></ul><p>为了实现以上<strong>探索和开发的平衡 (exploration-exploitation trade-off)</strong>，贝叶斯优化使用了<strong>采集函数 (acquisition function)</strong>，它能平衡好全局最小值的探索和开发。</p><blockquote><p>TODO 2: 迭代策略与采集函数</p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://banxian-w.com/article/2023/3/27/2539.html">https://banxian-w.com/article/2023/3/27/2539.html</a></p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://leovan.me/cn/2020/06/bayesian-optimization/">https://leovan.me/cn/2020/06/bayesian-optimization/</a></p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://mp.weixin.qq.com/s/waPWzo6iIEXYaH_MQdLfYg">https://mp.weixin.qq.com/s/waPWzo6iIEXYaH_MQdLfYg</a></p></blockquote><h2 id="Hyperband"><strong>Hyperband</strong></h2><p>TODO 3</p><blockquote><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.cnblogs.com/marsggbo/p/10161605.html">https://www.cnblogs.com/marsggbo/p/10161605.html</a></p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/jose_M/article/details/106313669">https://blog.csdn.net/jose_M/article/details/106313669</a></p></blockquote><h2 id="高斯过程回归">高斯过程回归</h2><p>疑似最切题的一种方法</p><p>TODO4</p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://banxian-w.com/article/2023/3/17/2522.html">https://banxian-w.com/article/2023/3/17/2522.html</a></p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://cloud.tencent.com/developer/article/1965403">https://cloud.tencent.com/developer/article/1965403</a></p><h2 id="一些执行超参数搜索的tips和tricks">一些执行超参数搜索的tips和tricks</h2><h3 id="1-Implementation">1. Implementation</h3><p>大型神经网络通常需要很长时间来训练，因此执行超参数搜索可能需要很多天/周的时间。记住这一点很重要，因为它会影响代码库的设计。一种特殊的设计是让一个worker不断地对随机超参数进行采样并执行优化。在训练期间，worker将跟踪每个epoch之后的validation performance，并将模型检查点（以及其他训练统计信息，如随时间的损失 the loss over time）写入文件，最好是在共享文件系统上。将validation performance直接包含在文件名中是很有用的，这样可以方便地检查和排序进度。然后还有第二个程序，我们称之为master，它在计算集群中启动或终止worker，还可以检查worker编写的检查点，绘制他们的训练统计数据等。</p><h3 id="2-Prefer-one-validation-fold-to-cross-validation">2. Prefer one validation fold to cross-validation</h3><p>在大多数情况下，一个大小合适的单一验证集大大简化了代码库，而不需要多次交叉验证。你会听到人们说他们“交叉验证”了一个参数，但很多时候都假设他们仍然只使用了一个验证集。</p><h3 id="3-Hyperparameter-ranges">3. Hyperparameter ranges</h3><p>在对数尺度上搜索超参数。例如，学习率的典型抽样如下<code>learning_rate = 10 ** uniform(-6, 1)</code>。也就是说，我们从 一个均匀分布中生成一个随机数，然后把它乘以10的幂。<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/weixin_44120025/article/details/114691964">正则化</a>强度也应采用相同的策略。直观地说，这是因为学习率和正则化强度对training dynamics有乘法效应（multiplicative effects）。例如，当学习率为0.001时，学习率加0.01的固定变化对dynamics有很大影响，而当学习率为10时，几乎没有影响。这是因为在更新中学习率和计算的<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/weixin_44120025/article/details/114748739">梯度</a>相乘了。因此，将一个范围内的学习率乘以或除以某个值，比将这个范围内的学习率乘以或减去某个值要合适得多。一些参数（如dropout）通常在原始比例（original scale）中搜索（如<code>dropout = uniform(0,1)</code>）。</p><h3 id="4-Prefer-random-search-to-grid-search">4. Prefer random search to grid search</h3><p>正如Bergstra and Bengio在Random Search for Hyper-Parameter Optimization上所说的：“randomly chosen trials比trials on a grid更有效”。事实证明，这通常也更容易实现。</p><p><img src="3.png" alt=""><br>通常情况下，某些超参数比其他超参数更重要（例如图中顶部的超参数与左侧的超参数相比)。相比于网格搜索，执行随机搜索可以更精确地发现重要值。</p><h3 id="5-Careful-with-best-values-on-border">5. Careful with best values on border</h3><p>有时可能发生的情况是，你正在一个坏的范围（bad range）内搜索一个超参数（如学习率）。例如，假设我们使用<code>learning_rate = 10 ** uniform(-6, 1)</code>。一旦我们获取到结果，重要的是要再次检查最终的学习率不在此间隔的边缘，否则你可能会错过超出此间隔的更优化的超参数设置。</p><h3 id="6-Stage-your-search-from-coarse-to-fine">6. Stage your search from coarse to fine</h3><p>在实践中，一个好的方法是首先在粗略范围（例如10**[-6，1]）中搜索，然后根据最佳结果出现的位置缩小范围。此外，在只训练1个epoch或更少epoch的情况下进行初始粗搜索也是有帮助的，因为许多超参数设置会导致模型根本不进行学习，或者因为无穷大的代价（infinite cost）而直接崩溃，代价是无限的。然后，第二阶段可以对5个epochs执行更窄的搜索，最后一阶段可以在最终范围内对更多epochs执行详细搜索。</p><h3 id="7-Bayesian-Hyperparameter-Optimization">7. Bayesian Hyperparameter Optimization</h3><p>贝叶斯超参数优化是一个致力于提出更有效地寻找超参数空间的算法研究领域。其核心思想是在查询不同超参数下的性能时，适当地平衡exploration - exploitation之间的权衡。在这些模型的基础上还开发了多个库，其中一些比较著名的是Spearmint、SMAC和Hyperopt。然而，在使用ConvNets的实际环境中，carefully-chosen intervals击败random search仍然是相对困难的。</p><h2 id="Reference">Reference</h2><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://cs231n.github.io/neural-networks-3/">https://cs231n.github.io/neural-networks-3/</a></p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/weixin_44120025/article/details/114677393">https://blog.csdn.net/weixin_44120025/article/details/114677393</a></p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://mp.weixin.qq.com/s/waPWzo6iIEXYaH_MQdLfYg">https://mp.weixin.qq.com/s/waPWzo6iIEXYaH_MQdLfYg</a></p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://zwn2001.space">琉璃月</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://zwn2001.space/posts/Graduate-Works/Course-Notes/%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0%E7%9A%84%E6%9C%80%E4%BC%98%E5%8C%96/">https://zwn2001.space/posts/Graduate-Works/Course-Notes/%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0%E7%9A%84%E6%9C%80%E4%BC%98%E5%8C%96/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://zwn2001.space" target="_blank">ZWN's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0/">学习</a></div><div class="post_share"><div class="social-share" data-image="/img/cover/33.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://unpkg.com/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://unpkg.com/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0%E7%9A%84%E6%9C%80%E4%BC%98%E5%8C%96/" title="默认参数的最优化"><img class="cover" src="/img/cover/55.jpg" onerror='onerror=null,src="/img/404.webp"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">默认参数的最优化</div></div></a></div><div class="next-post pull-right"><a href="/posts/Graduate-Works/Paper-Read/%E8%AF%BBpaper12-%E9%9D%99%E6%80%81%E8%AD%A6%E6%8A%A5%E8%87%AA%E5%8A%A8%E7%A1%AE%E8%AE%A4%E4%B8%8ELLM%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%80%A7%E8%AF%84%E4%BC%B0/" title="读paper12-静态警报自动确认与LLM代码生成的正确性评估"><img class="cover" src="/img/cover/29.jpg" onerror='onerror=null,src="/img/404.webp"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">读paper12-静态警报自动确认与LLM代码生成的正确性评估</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/Undergraduate-Course/%E7%A7%BB%E5%8A%A8%E4%BA%92%E8%81%94%E7%BD%91%E5%90%8D%E8%AF%8D/" title="移动互联网期末名词解释"><img class="cover" src="/img/cover/2.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-30</div><div class="title">移动互联网期末名词解释</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/favicon.webp" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">琉璃月</div><div class="author-info__description">我虽无意逐鹿，却知苍生苦楚</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">158</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">49</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/ZWN2001"><i class="fab fa-github"></i><span>我的Github</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ZWN2001" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="fab fa-github"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">新域名：www.zwn2001.space，有效期：10年。https://www.zwn-blog.xyz已过期。访问时建议科学上网，否则博客内公式渲染会出现问题且速度慢。Ctrl+shift+r可强制刷新网站以避免浏览器缓存造成的更新不及时</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">默认参数的最优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2-Grid-Search"><span class="toc-number">1.1.</span> <span class="toc-text">网格搜索(Grid Search)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%90%9C%E7%B4%A2-Randomized-Search"><span class="toc-number">1.2.</span> <span class="toc-text">随机搜索(Randomized Search)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96-Bayesian-Optimization"><span class="toc-number">1.3.</span> <span class="toc-text">贝叶斯优化(Bayesian Optimization)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hyperband"><span class="toc-number">1.4.</span> <span class="toc-text">Hyperband</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B%E5%9B%9E%E5%BD%92"><span class="toc-number">1.5.</span> <span class="toc-text">高斯过程回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E6%89%A7%E8%A1%8C%E8%B6%85%E5%8F%82%E6%95%B0%E6%90%9C%E7%B4%A2%E7%9A%84tips%E5%92%8Ctricks"><span class="toc-number">1.6.</span> <span class="toc-text">一些执行超参数搜索的tips和tricks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Implementation"><span class="toc-number">1.6.1.</span> <span class="toc-text">1. Implementation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Prefer-one-validation-fold-to-cross-validation"><span class="toc-number">1.6.2.</span> <span class="toc-text">2. Prefer one validation fold to cross-validation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Hyperparameter-ranges"><span class="toc-number">1.6.3.</span> <span class="toc-text">3. Hyperparameter ranges</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Prefer-random-search-to-grid-search"><span class="toc-number">1.6.4.</span> <span class="toc-text">4. Prefer random search to grid search</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Careful-with-best-values-on-border"><span class="toc-number">1.6.5.</span> <span class="toc-text">5. Careful with best values on border</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-Stage-your-search-from-coarse-to-fine"><span class="toc-number">1.6.6.</span> <span class="toc-text">6. Stage your search from coarse to fine</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-Bayesian-Hyperparameter-Optimization"><span class="toc-number">1.6.7.</span> <span class="toc-text">7. Bayesian Hyperparameter Optimization</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-number">1.7.</span> <span class="toc-text">Reference</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0%E7%9A%84%E6%9C%80%E4%BC%98%E5%8C%96/" title="默认参数的最优化"><img src="/img/cover/55.jpg" onerror='this.onerror=null,this.src="/img/404.webp"' alt="默认参数的最优化"></a><div class="content"><a class="title" href="/posts/%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0%E7%9A%84%E6%9C%80%E4%BC%98%E5%8C%96/" title="默认参数的最优化">默认参数的最优化</a><time datetime="2024-10-30T02:57:03.000Z" title="发表于 2024-10-30 10:57:03">2024-10-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/Graduate-Works/Course-Notes/%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0%E7%9A%84%E6%9C%80%E4%BC%98%E5%8C%96/" title="默认参数的最优化"><img src="/img/cover/33.jpg" onerror='this.onerror=null,this.src="/img/404.webp"' alt="默认参数的最优化"></a><div class="content"><a class="title" href="/posts/Graduate-Works/Course-Notes/%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0%E7%9A%84%E6%9C%80%E4%BC%98%E5%8C%96/" title="默认参数的最优化">默认参数的最优化</a><time datetime="2024-10-30T02:57:03.000Z" title="发表于 2024-10-30 10:57:03">2024-10-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/Graduate-Works/Paper-Read/%E8%AF%BBpaper12-%E9%9D%99%E6%80%81%E8%AD%A6%E6%8A%A5%E8%87%AA%E5%8A%A8%E7%A1%AE%E8%AE%A4%E4%B8%8ELLM%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%80%A7%E8%AF%84%E4%BC%B0/" title="读paper12-静态警报自动确认与LLM代码生成的正确性评估"><img src="/img/cover/29.jpg" onerror='this.onerror=null,this.src="/img/404.webp"' alt="读paper12-静态警报自动确认与LLM代码生成的正确性评估"></a><div class="content"><a class="title" href="/posts/Graduate-Works/Paper-Read/%E8%AF%BBpaper12-%E9%9D%99%E6%80%81%E8%AD%A6%E6%8A%A5%E8%87%AA%E5%8A%A8%E7%A1%AE%E8%AE%A4%E4%B8%8ELLM%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%80%A7%E8%AF%84%E4%BC%B0/" title="读paper12-静态警报自动确认与LLM代码生成的正确性评估">读paper12-静态警报自动确认与LLM代码生成的正确性评估</a><time datetime="2024-10-28T10:21:10.000Z" title="发表于 2024-10-28 18:21:10">2024-10-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/Graduate-Works/Paper-Read/AutoGen-multiple%20agents%20framework/" title="AutoGen-multiple agents framework"><img src="/img/cover/18.jpg" onerror='this.onerror=null,this.src="/img/404.webp"' alt="AutoGen-multiple agents framework"></a><div class="content"><a class="title" href="/posts/Graduate-Works/Paper-Read/AutoGen-multiple%20agents%20framework/" title="AutoGen-multiple agents framework">AutoGen-multiple agents framework</a><time datetime="2024-10-27T13:29:47.000Z" title="发表于 2024-10-27 21:29:47">2024-10-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/Graduate-Works/Paper-Read/%E8%AF%BBpaper11-%E5%9F%BA%E4%BA%8ELLM%E7%9A%84%E7%BC%BA%E9%99%B7%E4%BF%AE%E5%A4%8D3/" title="读paper11-基于LLM的缺陷修复3"><img src="/img/cover/32.jpg" onerror='this.onerror=null,this.src="/img/404.webp"' alt="读paper11-基于LLM的缺陷修复3"></a><div class="content"><a class="title" href="/posts/Graduate-Works/Paper-Read/%E8%AF%BBpaper11-%E5%9F%BA%E4%BA%8ELLM%E7%9A%84%E7%BC%BA%E9%99%B7%E4%BF%AE%E5%A4%8D3/" title="读paper11-基于LLM的缺陷修复3">读paper11-基于LLM的缺陷修复3</a><time datetime="2024-10-22T08:18:16.000Z" title="发表于 2024-10-22 16:18:16">2024-10-22</time></div></div></div></div></div></div></main><footer id="footer" style="background:0 0"></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span> 数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div><div class="js-pjax" id="rightMenu"><div class="rightMenu-group rightMenu-small"><a class="rightMenu-item" href="javascript:window.history.back();" rel="external nofollow noreferrer"><i class="fa fa-arrow-left"></i></a><a class="rightMenu-item" href="javascript:window.history.forward();" rel="external nofollow noreferrer"><i class="fa fa-arrow-right"></i></a><a class="rightMenu-item" href="javascript:window.location.reload();" rel="external nofollow noreferrer"><i class="fa fa-refresh"></i></a><a class="rightMenu-item" href="javascript:rmf.scrollToTop();" rel="external nofollow noreferrer"><i class="fa fa-arrow-up"></i></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-text"><a class="rightMenu-item" href="javascript:rmf.copySelect();" rel="external nofollow noreferrer"><i class="fa fa-copy"></i><span>复制</span></a><a class="rightMenu-item" href="javascript:window.open(&quot;https://www.google.com/search?q=&quot;+window.getSelection().toString());" rel="external nofollow noreferrer"><i class="iconfont icon-baidu"></i><span>搜索</span></a><a class="rightMenu-item" href="javascript:rmf.searchinThisPage();" rel="external nofollow noreferrer"><i class="fas fa-search"></i><span>站内搜索</span></a><a class="rightMenu-item" href="#post-comment" onclick="rmf.yinyong()"><i class="fa-solid fa-message"></i><span>引用文本评论</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-too"><a class="rightMenu-item" href="javascript:window.open(window.getSelection().toString());window.location.reload();" rel="external nofollow noreferrer"><i class="fa fa-link"></i><span>转到链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-paste"><a class="rightMenu-item" href="javascript:rmf.paste()" rel="external nofollow noreferrer"><i class="fa fa-copy"></i><span>粘贴</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-to"><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()" rel="external nofollow noreferrer"><i class="fa fa-window-restore"></i><span>新窗口打开</span></a><a class="rightMenu-item" id="menu-too" href="javascript:rmf.open()" rel="external nofollow noreferrer"><i class="fa fa-link"></i><span>转到链接</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()" rel="external nofollow noreferrer"><i class="fa fa-copy"></i><span>复制链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-img"><a class="rightMenu-item" href="javascript:rmf.saveAs()" rel="external nofollow noreferrer"><i class="fa fa-download"></i><span>保存图片</span></a><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()" rel="external nofollow noreferrer"><i class="fa fa-window-restore"></i><span>在新窗口打开</span></a><a class="rightMenu-item" href="javascript:rmf.click()" rel="external nofollow noreferrer"><i class="fa fa-arrows-alt"></i><span>全屏显示</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()" rel="external nofollow noreferrer"><i class="fa fa-copy"></i><span>复制图片链接</span></a></div><div class="rightMenu-group rightMenu-line"><a class="rightMenu-item" href="javascript:rmf.switchDarkMode();" rel="external nofollow noreferrer"><i class="fa fa-moon"></i><span>昼夜切换</span></a><a class="rightMenu-item" href="javascript:rmf.translate();" rel="external nofollow noreferrer"><i class="iconfont icon-fanti"></i><span>繁简转换</span></a><a class="rightMenu-item" href="javascript:rmf.switchReadMode();" rel="external nofollow noreferrer"><i class="fa fa-book"></i><span>阅读模式</span></a><a class="rightMenu-item" href="javascript:fullScreen();" rel="external nofollow noreferrer"><i class="fas fa-expand"></i><span>进入全屏</span></a></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://unpkg.com/@fancyapps/ui/dist/fancybox/fancybox.umd.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://unpkg.com/pangu/dist/browser/pangu.min.js").then(()=>{pangu.autoSpacingPage()})}function panguInit(){GLOBAL_CONFIG_SITE.isPost&&panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://unpkg.com/katex/dist/katex.min.css"><script src="https://unpkg.com/katex/dist/contrib/copy-tex.min.js"></script><script>document.querySelectorAll("#article-container span.katex-display").forEach(a=>{btf.wrap(a,"div",{class:"katex-wrap"})})</script><script>function getGiscusTheme(e){return"dark"===e?"dark":"light"}function loadGiscus(){var e,t=Object.assign({src:"https://giscus.app/client.js","data-repo":"ZWN2001/ZWN2001.github.io","data-repo-id":"R_kgDOGH1XWg","data-category-id":"DIC_kwDOGH1XWs4CXnHJ","data-mapping":"pathname","data-theme":getGiscusTheme(document.documentElement.getAttribute("data-theme")),"data-reactions-enabled":"1",crossorigin:"anonymous",async:!0},{"data-lang":"zh-CN","data-loading":"lazy",crossorigin:"anonymous","data-mapping":"og:title","data-input-position":"top","data-category":"Announcements"}),a=document.createElement("script");for(e in t)a.setAttribute(e,t[e]);document.getElementById("giscus-wrap").insertAdjacentElement("afterbegin",a)}function changeGiscusTheme(e){var t;e={setConfig:{theme:getGiscusTheme(e)}},(t=document.querySelector("iframe.giscus-frame"))&&t.contentWindow.postMessage({giscus:e},"https://giscus.app")}function loadOtherComment(){loadGiscus()}btf.addModeChange("giscus",changeGiscusTheme),btf.loadComment(document.getElementById("giscus-wrap"),loadGiscus)</script></div><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script><script type="text/javascript" src="/js/rightmenu.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span> 数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script data-pjax>var parent,child;document.getElementById("recent-posts")&&"/"===location.pathname&&(parent=document.getElementById("recent-posts"),child='<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="https://zwn2001.space/categories/编程知识/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 琉璃月の编程知识 (13)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://zwn2001.space/categories/实用知识/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">💡 琉璃月の实用知识 (10)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://zwn2001.space/categories/学习-课外拓展/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">👩‍💻 琉璃月の学习-课外拓展 (33)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://zwn2001.space/categories/学习-课内知识/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📒 琉璃月の学习-课内知识 (58)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="https://zwn2001.space/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>',console.log("已挂载magnet"),parent.insertAdjacentHTML("afterbegin",child))</script><style>#catalog_magnet{flex-wrap:wrap;display:flex;width:100%;justify-content:space-between;padding:10px 10px 0 10px;align-content:flex-start}.magnet_item{flex-basis:calc(50% - 5px);background:#f2f2f2;margin-bottom:10px;border-radius:8px;transition:all .2s ease-in-out}.magnet_item:hover{background:#b30070}.magnet_link_more{color:#555}.magnet_link{color:#000}.magnet_link:hover{color:#fff}@media screen and (max-width:600px){.magnet_item{flex-basis:100%}}.magnet_link_context{display:flex;padding:10px;font-size:16px;transition:all .2s ease-in-out}.magnet_link_context:hover{padding:10px 20px}</style><style></style></body></html>